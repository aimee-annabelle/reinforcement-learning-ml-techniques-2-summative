{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1340a293",
   "metadata": {},
   "source": [
    "# Rwanda Health RL Training Notebook\n",
    "\n",
    "End-to-end experimentation pipeline for DQN, REINFORCE, PPO, and A2C on the clinic environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f5e66",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "1. Open this notebook in Google Colab or any GPU-enabled environment and make sure the repo is available (e.g., run `!git clone <repo-url>` so `environment.custom_env` can be imported).\n",
    "2. Run the dependency cell if the runtime is fresh.\n",
    "3. Configure the toggles (`RUN_DQN`, `RUN_PPO`, `RUN_A2C`, `RUN_REINFORCE`) before launching training.\n",
    "4. After runs finish, execute the results and visualization cells to compare agents.\n",
    "5. Export the final table to CSV for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52573526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when running on a fresh Colab runtime\n",
    "# !pip install --quiet \"gymnasium==0.29.1\" \"stable-baselines3==2.3.2\" \"sb3-contrib==2.3.2\" \"pygame==2.6.0\"\n",
    "# !pip install --quiet \"torch==2.3.0\" \"numpy\" \"pandas\" \"matplotlib\" \"seaborn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results')\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, 'logs')\n",
    "\n",
    "for subdir in [MODEL_DIR, RESULTS_DIR, LOG_DIR]:\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "for algo_name in ['dqn', 'ppo', 'a2c', 'reinforce']:\n",
    "    os.makedirs(os.path.join(MODEL_DIR, algo_name), exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dba457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.custom_env import (\n",
    "    RwandaHealthEnv,\n",
    "    ACTION_MEANINGS,\n",
    "    REQUEST_NCD_TEST,\n",
    "    REQUEST_INFECTION_TEST,\n",
    "    DIAGNOSE_CHRONIC,\n",
    "    DIAGNOSE_INFECTION,\n",
    "    ALLOCATE_MED,\n",
    "    REFER_PATIENT,\n",
    "    WAIT,\n",
    "    make_env as _base_make_env,\n",
    "    CONDITION_HEALTHY_OR_MILD,\n",
    "    CONDITION_CHRONIC,\n",
    "    CONDITION_INFECTION,\n",
    "    CONDITION_BOTH_SERIOUS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_runs: List[Dict[str, Any]] = []\n",
    "\n",
    "def make_env(seed: Optional[int] = None, render_mode: Optional[str] = None, monitor: bool = True):\n",
    "    return _base_make_env(seed=seed, render_mode=render_mode, monitor=monitor)\n",
    "\n",
    "def evaluate_sb3_model(model, eval_episodes: int = 20, seed: int = 10_000) -> Tuple[float, float]:\n",
    "    eval_env = make_env(seed=seed, monitor=False)()\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=eval_episodes, deterministic=True)\n",
    "    eval_env.close()\n",
    "    return float(mean_reward), float(std_reward)\n",
    "\n",
    "def evaluate_reinforce_policy(policy: nn.Module, eval_episodes: int = 20, seed: int = 20_000) -> Tuple[float, float]:\n",
    "    policy.eval()\n",
    "    rewards: List[float] = []\n",
    "    for episode in range(eval_episodes):\n",
    "        env = make_env(seed=seed + episode, monitor=False)()\n",
    "        obs, _ = env.reset(seed=seed + episode)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        episode_reward = 0.0\n",
    "        while not (terminated or truncated):\n",
    "            obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits = policy(obs_tensor)\n",
    "            action = int(torch.argmax(logits).item())\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        env.close()\n",
    "        rewards.append(episode_reward)\n",
    "    policy.train()\n",
    "    return float(np.mean(rewards)), float(np.std(rewards))\n",
    "\n",
    "def record_result(\n",
    "    algorithm: str,\n",
    "    run_id: int,\n",
    "    seed: int,\n",
    "    params: Dict[str, Any],\n",
    "    mean_reward: float,\n",
    "    std_reward: float,\n",
    "    metadata: Optional[Dict[str, Any]] = None,\n",
    " ) -> None:\n",
    "    run_record = {\n",
    "        'algorithm': algorithm,\n",
    "        'run_id': run_id,\n",
    "        'seed': seed,\n",
    "        'mean_reward': mean_reward,\n",
    "        'std_reward': std_reward,\n",
    "        'hyperparameters': dict(params),\n",
    "    }\n",
    "    if metadata:\n",
    "        run_record.update(metadata)\n",
    "    experiment_runs.append(run_record)\n",
    "\n",
    "dummy_env = RwandaHealthEnv()\n",
    "OBS_DIM = dummy_env.observation_space.shape[0]\n",
    "ACT_DIM = dummy_env.action_space.n\n",
    "dummy_env.close()\n",
    "del dummy_env\n",
    "print(f'Observation dim: {OBS_DIM}, action dim: {ACT_DIM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dqn_grid() -> List[Dict[str, Any]]:\n",
    "    combos = list(itertools.product(\n",
    "        [1e-3, 5e-4, 3e-4],\n",
    "        [0.95, 0.98, 0.99],\n",
    "        [4096, 8192, 16384],\n",
    "        [32, 64, 128],\n",
    "        [1, 4, 8],\n",
    "        [500, 750, 1000],\n",
    "        [0.2, 0.3],\n",
    "        [0.01, 0.02],\n",
    "    ))\n",
    "    grid: List[Dict[str, Any]] = []\n",
    "    for combo in combos:\n",
    "        lr, gamma, buffer_size, batch_size, train_freq, target_update_interval, exploration_fraction, exploration_final_eps = combo\n",
    "        grid.append({\n",
    "            'learning_rate': lr,\n",
    "            'gamma': gamma,\n",
    "            'buffer_size': buffer_size,\n",
    "            'batch_size': batch_size,\n",
    "            'train_freq': train_freq,\n",
    "            'target_update_interval': target_update_interval,\n",
    "            'exploration_fraction': exploration_fraction,\n",
    "            'exploration_final_eps': exploration_final_eps,\n",
    "        })\n",
    "        if len(grid) >= 12:\n",
    "            break\n",
    "    return grid\n",
    "\n",
    "def build_ppo_grid() -> List[Dict[str, Any]]:\n",
    "    combos = list(itertools.product(\n",
    "        [3e-4, 1e-4, 5e-4],\n",
    "        [0.95, 0.98, 0.99],\n",
    "        [1024, 2048, 3072],\n",
    "        [64, 128, 256],\n",
    "        [3, 5, 10],\n",
    "        [0.9, 0.95, 0.99],\n",
    "        [0.1, 0.2, 0.3],\n",
    "    ))\n",
    "    grid: List[Dict[str, Any]] = []\n",
    "    for combo in combos:\n",
    "        lr, gamma, n_steps, batch_size, n_epochs, gae_lambda, clip_range = combo\n",
    "        if batch_size > n_steps:\n",
    "            continue\n",
    "        grid.append({\n",
    "            'learning_rate': lr,\n",
    "            'gamma': gamma,\n",
    "            'n_steps': n_steps,\n",
    "            'batch_size': batch_size,\n",
    "            'n_epochs': n_epochs,\n",
    "            'gae_lambda': gae_lambda,\n",
    "            'clip_range': clip_range,\n",
    "        })\n",
    "        if len(grid) >= 12:\n",
    "            break\n",
    "    return grid\n",
    "\n",
    "def build_a2c_grid() -> List[Dict[str, Any]]:\n",
    "    combos = list(itertools.product(\n",
    "        [7e-4, 5e-4, 3e-4],\n",
    "        [0.95, 0.98, 0.99],\n",
    "        [0.9, 0.95, 0.99],\n",
    "        [5, 10, 20],\n",
    "        [0.0, 0.01, 0.05],\n",
    "    ))\n",
    "    grid: List[Dict[str, Any]] = []\n",
    "    for combo in combos:\n",
    "        lr, gamma, gae_lambda, n_steps, ent_coef = combo\n",
    "        grid.append({\n",
    "            'learning_rate': lr,\n",
    "            'gamma': gamma,\n",
    "            'gae_lambda': gae_lambda,\n",
    "            'n_steps': n_steps,\n",
    "            'ent_coef': ent_coef,\n",
    "        })\n",
    "        if len(grid) >= 12:\n",
    "            break\n",
    "    return grid\n",
    "\n",
    "def build_reinforce_grid() -> List[Dict[str, Any]]:\n",
    "    combos = list(itertools.product(\n",
    "        [1e-3, 5e-4, 3e-4],\n",
    "        [0.95, 0.98, 0.99],\n",
    "        [(128, 128), (256, 128), (256, 256), (128, 64)],\n",
    "        [0.0, 0.01, 0.02],\n",
    "        [0.5, 1.0],\n",
    "        [20, 30, 40],\n",
    "    ))\n",
    "    grid: List[Dict[str, Any]] = []\n",
    "    for combo in combos:\n",
    "        lr, gamma, hidden_layers, entropy_coef, max_grad_norm, log_interval = combo\n",
    "        grid.append({\n",
    "            'learning_rate': lr,\n",
    "            'gamma': gamma,\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'entropy_coef': entropy_coef,\n",
    "            'max_grad_norm': max_grad_norm,\n",
    "            'log_interval': log_interval,\n",
    "        })\n",
    "        if len(grid) >= 12:\n",
    "            break\n",
    "    return grid\n",
    "\n",
    "DQN_PARAM_GRID = build_dqn_grid()\n",
    "PPO_PARAM_GRID = build_ppo_grid()\n",
    "A2C_PARAM_GRID = build_a2c_grid()\n",
    "REINFORCE_PARAM_GRID = build_reinforce_grid()\n",
    "\n",
    "print(f'DQN grid size: {len(DQN_PARAM_GRID)}')\n",
    "print(f'PPO grid size: {len(PPO_PARAM_GRID)}')\n",
    "print(f'A2C grid size: {len(A2C_PARAM_GRID)}')\n",
    "print(f'REINFORCE grid size: {len(REINFORCE_PARAM_GRID)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = {\n",
    "    'dqn': 200_000,\n",
    "    'ppo': 600_000,\n",
    "    'a2c': 400_000,\n",
    "}\n",
    "\n",
    "REINFORCE_EPISODES = 800\n",
    "EVAL_EPISODES = 20\n",
    "BASE_SEED = 2024\n",
    "\n",
    "print('Timesteps per algorithm:', TOTAL_TIMESTEPS)\n",
    "print(f'Reinforce episodes: {REINFORCE_EPISODES}, evaluation episodes: {EVAL_EPISODES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c050aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sb3_experiments(\n",
    "    algo_name: str,\n",
    "    algo_cls,\n",
    "    param_grid: List[Dict[str, Any]],\n",
    "    total_timesteps: int,\n",
    "    eval_episodes: int = EVAL_EPISODES,\n",
    "    base_seed: int = BASE_SEED,\n",
    " ) -> None:\n",
    "    print(f'Starting {algo_name.upper()} experiments ({len(param_grid)} runs)...')\n",
    "    for run_idx, params in enumerate(param_grid):\n",
    "        run_seed = base_seed + run_idx\n",
    "        set_random_seed(run_seed)\n",
    "        env = DummyVecEnv([make_env(seed=run_seed, monitor=True)])\n",
    "        log_dir = os.path.join(LOG_DIR, algo_name, f'run_{run_idx:02d}')\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        model = algo_cls('MlpPolicy', env, verbose=1, tensorboard_log=log_dir, seed=run_seed, **params)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "        except TypeError:\n",
    "            model.learn(total_timesteps=total_timesteps)\n",
    "        duration = time.time() - start_time\n",
    "        mean_reward, std_reward = evaluate_sb3_model(model, eval_episodes=eval_episodes, seed=run_seed + 10_000)\n",
    "        model_path = os.path.join(MODEL_DIR, algo_name, f'{algo_name}_run_{run_idx:02d}')\n",
    "        model.save(model_path)\n",
    "        env.close()\n",
    "        record_result(\n",
    "            algorithm=algo_name,\n",
    "            run_id=run_idx,\n",
    "            seed=run_seed,\n",
    "            params=params,\n",
    "            mean_reward=mean_reward,\n",
    "            std_reward=std_reward,\n",
    "            metadata={'duration_sec': duration, 'timesteps': total_timesteps, 'model_path': model_path},\n",
    "        )\n",
    "        print(f'Run {run_idx:02d} -> mean reward {mean_reward:.2f} ± {std_reward:.2f} (saved to {model_path})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DQN = False\n",
    "if RUN_DQN:\n",
    "    run_sb3_experiments(\n",
    "        algo_name='dqn',\n",
    "        algo_cls=DQN,\n",
    "        param_grid=DQN_PARAM_GRID,\n",
    "        total_timesteps=TOTAL_TIMESTEPS['dqn'],\n",
    "        eval_episodes=EVAL_EPISODES,\n",
    "        base_seed=BASE_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PPO = False\n",
    "if RUN_PPO:\n",
    "    run_sb3_experiments(\n",
    "        algo_name='ppo',\n",
    "        algo_cls=PPO,\n",
    "        param_grid=PPO_PARAM_GRID,\n",
    "        total_timesteps=TOTAL_TIMESTEPS['ppo'],\n",
    "        eval_episodes=EVAL_EPISODES,\n",
    "        base_seed=BASE_SEED + 500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_A2C = False\n",
    "if RUN_A2C:\n",
    "    run_sb3_experiments(\n",
    "        algo_name='a2c',\n",
    "        algo_cls=A2C,\n",
    "        param_grid=A2C_PARAM_GRID,\n",
    "        total_timesteps=TOTAL_TIMESTEPS['a2c'],\n",
    "        eval_episodes=EVAL_EPISODES,\n",
    "        base_seed=BASE_SEED + 1_000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden_layers: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        input_dim = obs_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, act_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "def train_reinforce(\n",
    "    param_grid: List[Dict[str, Any]],\n",
    "    total_episodes: int = REINFORCE_EPISODES,\n",
    "    eval_episodes: int = EVAL_EPISODES,\n",
    "    base_seed: int = BASE_SEED + 2_000,\n",
    " ) -> None:\n",
    "    print(f'Starting REINFORCE experiments ({len(param_grid)} runs)...')\n",
    "    for run_idx, params in enumerate(param_grid):\n",
    "        run_seed = base_seed + run_idx\n",
    "        set_random_seed(run_seed)\n",
    "        np.random.seed(run_seed)\n",
    "        torch.manual_seed(run_seed)\n",
    "        env = make_env(seed=run_seed, monitor=False)()\n",
    "        policy = ReinforceNetwork(OBS_DIM, ACT_DIM, params['hidden_layers']).to(DEVICE)\n",
    "        policy.train()\n",
    "        optimizer = torch.optim.Adam(policy.parameters(), lr=params['learning_rate'])\n",
    "        gamma = params['gamma']\n",
    "        entropy_coef = params.get('entropy_coef', 0.0)\n",
    "        max_grad_norm = params.get('max_grad_norm', 1.0)\n",
    "        log_interval = params.get('log_interval', 20)\n",
    "        episode_rewards: List[float] = []\n",
    "        start_time = time.time()\n",
    "        for episode in range(total_episodes):\n",
    "            log_probs: List[torch.Tensor] = []\n",
    "            entropies: List[torch.Tensor] = []\n",
    "            rewards: List[float] = []\n",
    "            obs, _ = env.reset(seed=run_seed + episode)\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            while not (terminated or truncated):\n",
    "                obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "                dist = Categorical(logits=policy(obs_tensor))\n",
    "                action = dist.sample()\n",
    "                log_probs.append(dist.log_prob(action))\n",
    "                entropies.append(dist.entropy())\n",
    "                obs, reward, terminated, truncated, _ = env.step(int(action.item()))\n",
    "                rewards.append(reward)\n",
    "            returns: List[float] = []\n",
    "            discounted_return = 0.0\n",
    "            for reward in reversed(rewards):\n",
    "                discounted_return = reward + gamma * discounted_return\n",
    "                returns.insert(0, discounted_return)\n",
    "            returns_tensor = torch.as_tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "            normalized_returns = (returns_tensor - returns_tensor.mean()) / (returns_tensor.std() + 1e-8)\n",
    "            log_prob_tensor = torch.stack(log_probs)\n",
    "            entropy_tensor = torch.stack(entropies)\n",
    "            loss = -(normalized_returns.detach() * log_prob_tensor).sum()\n",
    "            if entropy_coef > 0.0:\n",
    "                loss -= entropy_coef * entropy_tensor.sum()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if max_grad_norm is not None:\n",
    "                clip_grad_norm_(policy.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            episode_reward = float(np.sum(rewards))\n",
    "            episode_rewards.append(episode_reward)\n",
    "            if (episode + 1) % log_interval == 0:\n",
    "                rolling = float(np.mean(episode_rewards[-log_interval:]))\n",
    "                print(f'Run {run_idx:02d} | Episode {episode + 1} | rolling reward {rolling:.2f}')\n",
    "        duration = time.time() - start_time\n",
    "        mean_reward, std_reward = evaluate_reinforce_policy(policy, eval_episodes=eval_episodes, seed=run_seed + 30_000)\n",
    "        model_path = os.path.join(MODEL_DIR, 'reinforce', f'reinforce_run_{run_idx:02d}.pt')\n",
    "        torch.save({'state_dict': policy.state_dict(), 'hyperparameters': dict(params)}, model_path)\n",
    "        env.close()\n",
    "        record_result(\n",
    "            algorithm='reinforce',\n",
    "            run_id=run_idx,\n",
    "            seed=run_seed,\n",
    "            params=params,\n",
    "            mean_reward=mean_reward,\n",
    "            std_reward=std_reward,\n",
    "            metadata={'duration_sec': duration, 'episodes': total_episodes, 'model_path': model_path},\n",
    "        )\n",
    "        print(f'Run {run_idx:02d} -> mean reward {mean_reward:.2f} ± {std_reward:.2f} (saved to {model_path})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_REINFORCE = False\n",
    "if RUN_REINFORCE:\n",
    "    train_reinforce(\n",
    "        param_grid=REINFORCE_PARAM_GRID,\n",
    "        total_episodes=REINFORCE_EPISODES,\n",
    "        eval_episodes=EVAL_EPISODES,\n",
    "        base_seed=BASE_SEED + 4_000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_runs:\n",
    "    results_df = pd.DataFrame(experiment_runs)\n",
    "    results_df = results_df.sort_values(by='mean_reward', ascending=False).reset_index(drop=True)\n",
    "    display(results_df)\n",
    "else:\n",
    "    print('No experiments recorded yet. Enable one of the RUN_* toggles and re-run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b526d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_runs:\n",
    "    results_df = pd.DataFrame(experiment_runs)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(data=results_df, x='algorithm', y='mean_reward')\n",
    "    plt.title('Reward distribution per algorithm')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.scatterplot(data=results_df, x='run_id', y='mean_reward', hue='algorithm', style='algorithm')\n",
    "    plt.title('Per-run rewards by algorithm')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No results to visualize yet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_runs:\n",
    "    results_df = pd.DataFrame(experiment_runs)\n",
    "    timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "    csv_path = os.path.join(RESULTS_DIR, f'rl_experiment_results_{timestamp}.csv')\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f'Results saved to {csv_path}')\n",
    "else:\n",
    "    print('Run at least one experiment before exporting results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_sb3_model(model_path: str, eval_episodes: int = EVAL_EPISODES) -> None:\n",
    "    if not os.path.exists(model_path) and os.path.exists(f'{model_path}.zip'):\n",
    "        model_path = f'{model_path}.zip'\n",
    "    model_path_lower = model_path.lower()\n",
    "    if 'dqn' in model_path_lower:\n",
    "        model = DQN.load(model_path)\n",
    "    elif 'ppo' in model_path_lower:\n",
    "        model = PPO.load(model_path)\n",
    "    elif 'a2c' in model_path_lower:\n",
    "        model = A2C.load(model_path)\n",
    "    else:\n",
    "        raise ValueError('Unsupported SB3 model path. Include algo name in filename.')\n",
    "    mean_reward, std_reward = evaluate_sb3_model(model, eval_episodes=eval_episodes, seed=BASE_SEED + 50_000)\n",
    "    print(f'Loaded {model.__class__.__name__} -> mean reward {mean_reward:.2f} ± {std_reward:.2f}')\n",
    "\n",
    "def evaluate_saved_reinforce(model_path: str, eval_episodes: int = EVAL_EPISODES) -> None:\n",
    "    payload = torch.load(model_path, map_location=DEVICE)\n",
    "    params = payload.get('hyperparameters', {'hidden_layers': (128, 128)})\n",
    "    hidden_layers = tuple(params.get('hidden_layers', (128, 128)))\n",
    "    policy = ReinforceNetwork(OBS_DIM, ACT_DIM, hidden_layers).to(DEVICE)\n",
    "    policy.load_state_dict(payload['state_dict'])\n",
    "    mean_reward, std_reward = evaluate_reinforce_policy(policy, eval_episodes=eval_episodes, seed=BASE_SEED + 60_000)\n",
    "    print(f'Loaded REINFORCE policy -> mean reward {mean_reward:.2f} ± {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c99517",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Flip one toggle at a time and monitor the console for each run.\n",
    "- Use TensorBoard logs saved under `logs/` for qualitative diagnostics.\n",
    "- Export the aggregated CSV and figures for the PDF report and presentation assets.\n",
    "- Record the best agent by loading the saved model and running `env.render()` with `render_mode='human'`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
