"""Vanilla REINFORCE training utilities for the RwandaHealthEnv environment."""

from __future__ import annotations

import csv
import random
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Sequence, Tuple

import numpy as np
import torch
from torch import Tensor, nn, optim

from environment.custom_env import RwandaHealthEnv


ACTIVATION_FNS = {
    "relu": nn.ReLU,
    "tanh": nn.Tanh,
    "elu": nn.ELU,
}


@dataclass
class ReinforceRunConfig:
    """Configuration bundle for a single REINFORCE experiment."""

    learning_rate: float
    gamma: float
    hidden_layers: Sequence[int]
    entropy_coef: float
    normalize_returns: bool
    episodes: int
    batch_episodes: int
    max_grad_norm: float
    activation: str = "relu"


@dataclass
class ReinforceRunResult:
    """Stores artefacts generated by a single REINFORCE run."""

    mean_eval_reward: float
    std_eval_reward: float
    best_smoothed_reward: float
    episode_rewards_path: Path
    policy_path: Path
    raw_rewards: List[float]
    smoothed_rewards: List[float]


class ReinforcePolicy(nn.Module):
    """Simple feed-forward policy producing a categorical action distribution."""

    def __init__(self, obs_dim: int, action_dim: int, hidden_layers: Sequence[int], activation: str) -> None:
        super().__init__()
        if len(hidden_layers) == 0:
            raise ValueError("hidden_layers must contain at least one entry for this setup")
        activation_cls = ACTIVATION_FNS.get(activation.lower())
        if activation_cls is None:
            raise ValueError(f"Unsupported activation '{activation}' for REINFORCE policy")

        layers: List[nn.Module] = []
        input_dim = obs_dim
        for hidden_dim in hidden_layers:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(activation_cls())
            input_dim = hidden_dim
        layers.append(nn.Linear(input_dim, action_dim))
        self.backbone = nn.Sequential(*layers)

    def forward(self, observation: Tensor) -> torch.distributions.Categorical:
        if observation.dim() == 1:
            observation = observation.unsqueeze(0)
        logits = self.backbone(observation)
        return torch.distributions.Categorical(logits=logits)


def _discount_rewards(rewards: Sequence[float], gamma: float) -> List[float]:
    discounted: List[float] = []
    g = 0.0
    for reward in reversed(rewards):
        g = reward + gamma * g
        discounted.insert(0, g)
    return discounted


def _set_global_seeds(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def _write_curve_csv(path: Path, rewards: Iterable[float], smoothed: Iterable[float]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.writer(handle)
        writer.writerow(["episode", "return", "ema_reward"])
        for idx, (ret, ema) in enumerate(zip(rewards, smoothed), start=1):
            writer.writerow([idx, f"{ret:.6f}", f"{ema:.6f}"])


def evaluate_policy(policy: ReinforcePolicy, episodes: int, seed_offset: int, base_seed: int, device: torch.device) -> Tuple[float, float]:
    policy.eval()
    returns: List[float] = []
    for episode in range(episodes):
        env = RwandaHealthEnv()
        obs, _ = env.reset(seed=base_seed + seed_offset + episode)
        done = False
        total_reward = 0.0
        with torch.no_grad():
            while not done:
                tensor_obs = torch.as_tensor(obs, dtype=torch.float32, device=device)
                distribution = policy(tensor_obs)
                action = int(torch.argmax(distribution.probs).item())
                obs, reward, terminated, truncated, _ = env.step(action)
                total_reward += reward
                done = terminated or truncated
        env.close()
        returns.append(total_reward)
    mean_reward = float(np.mean(returns)) if returns else 0.0
    std_reward = float(np.std(returns)) if returns else 0.0
    policy.train()
    return mean_reward, std_reward


def train_reinforce(
    config_dict: Dict[str, Any],
    run_directory: Path,
    seed: int,
    eval_episodes: int,
    device: torch.device,
) -> ReinforceRunResult:
    config = ReinforceRunConfig(**config_dict)
    _set_global_seeds(seed)

    env = RwandaHealthEnv()
    obs, _ = env.reset(seed=seed)
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = ReinforcePolicy(obs_dim, action_dim, config.hidden_layers, config.activation).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=config.learning_rate)

    episode_rewards: List[float] = []
    smoothed_rewards: List[float] = []
    best_smoothed_reward = float("-inf")

    entropy_coef = config.entropy_coef
    gamma = config.gamma
    batch_size = max(1, config.batch_episodes)
    max_grad_norm = config.max_grad_norm

    start_time = time.perf_counter()

    for episode_index in range(config.episodes):
        batch_log_probs: List[Tensor] = []
        batch_returns: List[Tensor] = []
        batch_entropies: List[Tensor] = []
        batch_totals: List[float] = []

        for batch_offset in range(batch_size):
            episode_seed = seed + episode_index * batch_size + batch_offset
            obs, _ = env.reset(seed=episode_seed)
            done = False
            log_probs: List[Tensor] = []
            rewards: List[float] = []
            entropies: List[Tensor] = []

            while not done:
                tensor_obs = torch.as_tensor(obs, dtype=torch.float32, device=device)
                distribution = policy(tensor_obs)
                action = distribution.sample()
                log_prob = distribution.log_prob(action)
                entropy = distribution.entropy()

                obs, reward, terminated, truncated, _ = env.step(int(action.item()))
                log_probs.append(log_prob)
                entropies.append(entropy)
                rewards.append(float(reward))

                done = terminated or truncated

            discounted = _discount_rewards(rewards, gamma)
            batch_log_probs.append(torch.stack(log_probs))
            batch_entropies.append(torch.stack(entropies))
            batch_returns.append(torch.tensor(discounted, dtype=torch.float32, device=device))
            batch_totals.append(sum(rewards))

        episode_rewards.extend(batch_totals)
        for total_return in batch_totals:
            if smoothed_rewards:
                smoothed = 0.9 * smoothed_rewards[-1] + 0.1 * total_return
            else:
                smoothed = total_return
            smoothed_rewards.append(smoothed)
            best_smoothed_reward = max(best_smoothed_reward, smoothed)

        stacked_returns = torch.cat(batch_returns)
        if config.normalize_returns and stacked_returns.numel() > 1:
            mean_return = stacked_returns.mean()
            std_return = stacked_returns.std(unbiased=False).clamp_min(1e-8)
        else:
            mean_return = torch.tensor(0.0, device=device)
            std_return = torch.tensor(1.0, device=device)

        losses: List[Tensor] = []
        offset = 0
        for log_probs, returns, entropies in zip(batch_log_probs, batch_returns, batch_entropies):
            if config.normalize_returns and stacked_returns.numel() > 1:
                returns = (returns - mean_return) / std_return
            policy_loss = -(log_probs * returns.detach()).sum()
            entropy_bonus = -(entropy_coef * entropies.sum()) if entropy_coef > 0.0 else torch.tensor(0.0, device=device)
            losses.append(policy_loss + entropy_bonus)
            offset += len(returns)

        loss = torch.stack(losses).mean()
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)
        optimizer.step()

    env.close()
    wall_clock = time.perf_counter() - start_time

    episode_rewards_path = run_directory / "episode_rewards.csv"
    _write_curve_csv(episode_rewards_path, episode_rewards, smoothed_rewards)

    policy_path = run_directory / "policy.pt"
    run_directory.mkdir(parents=True, exist_ok=True)
    torch.save({
        "state_dict": policy.state_dict(),
        "config": config_dict,
        "wall_clock_seconds": wall_clock,
    }, policy_path)

    mean_eval_reward, std_eval_reward = evaluate_policy(policy, eval_episodes, seed_offset=10_000, base_seed=seed, device=device)

    return ReinforceRunResult(
        mean_eval_reward=mean_eval_reward,
        std_eval_reward=std_eval_reward,
        best_smoothed_reward=best_smoothed_reward,
        episode_rewards_path=episode_rewards_path,
        policy_path=policy_path,
        raw_rewards=episode_rewards,
        smoothed_rewards=smoothed_rewards,
    )


__all__ = [
    "ReinforceRunConfig",
    "ReinforceRunResult",
    "ReinforcePolicy",
    "train_reinforce",
    "evaluate_policy",
]
